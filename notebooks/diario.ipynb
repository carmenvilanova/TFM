{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f05174f2",
      "metadata": {
        "id": "f05174f2"
      },
      "source": [
        "# DIARIO DE PROGRESO #"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8d8acd",
      "metadata": {
        "id": "ff8d8acd"
      },
      "source": [
        "### 27/06 Carmen ###\n",
        "\n",
        "Hola chicos! Ya he iniciado el proyecto :) Me pareció buena idea ir haciendo esta especie de diario para que, ya que somos muchos, podamos escribir el progreso que hemos hecho cada vez que hacemos un commit. Me parece más fácil que andar investigando el código o escribir por WhatsApp. Ya sé que da un poco de pereza y no sé qué tal funcionará pero vamos a intentarlo. Empiezo contandoos yo:\n",
        "\n",
        "De momento he empezado haciendo más o menos lo que tenía yo hecho en el trabajo, aunque solo con las convocatorias españolas, todavía no me puse con las europeas.\n",
        "\n",
        "He estado investigando un poco la documentación de la API, os la dejo por aquí: https://www.infosubvenciones.es/bdnstrans/doc/swagger y he visto que hay tres tipos de llamadas que son interesantes: a la lista general de convocatorias, a las convocatorias en específico y a los documentos. Creo que lo he dejado bien explicado en el notebook.\n",
        "\n",
        "En cuanto a posibles tareas por hacer:\n",
        "\n",
        "- Empezar a trabajar en la interfaz de usuario para hacer la búsqueda y consultar las convocatorias. El objetivo de esto es que sea mejor que la que ya existe en el SNPSAP (no es difícil). A mi en el futuro me gustaría llegar a algo más que una demo (me gustaría desplegarlo en una página web real) pero de momento podemos ir trabajando con Streamlit o Gradio.\n",
        "\n",
        "- Empezar a trabajar en la parte de NLP. Existen dos fases en mi opinión, una de la búsqueda principal (\"Hola, quiero buscar convocatorias relacionadas con ayuda a la vivienda\") y otra parte de RAG que sería a partir de los documentos de la convocatoria (\"Dime la lista de posibles beneficiarios según las bases de la convocatoria\"). La primera fase consistiría en pasar del lenguaje natural a una lista de parámetros para llamar a la API así que se podría ir haciendo y la segunda también, practicando con documentos ya descargados. Y luego ya sería juntarlo todo.\n",
        "\n",
        "- Limpiar el código, hacer un flujo que quede bonito, que tengan sentidos los documentos, etc. Es una tarea un poco más rollo pero igualmente importante.\n",
        "\n",
        "Creo que eso es todo, me parece que lo mejor es que cuando alguien se meta y progrese algo, lo documente e indique posibles tareas para que haga el siguiente, por lo menos al principio, luego ya podemos ir definiendo roles más claros. Y avisad por el grupo si alguien se pone para no estar trabajando en lo mismo!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pOB7rmN_6Rw6",
      "metadata": {
        "id": "pOB7rmN_6Rw6"
      },
      "source": [
        "30/06 Alex\n",
        "\n",
        "Ya creé el RAG. Lo que hace es que utiliza el pdf de la convocatoria que descargó Carmen, lo pasa a texto para poder procesarlo,  lo divide en embbedings y luego se le hacen las preguntas. Creo que a esto habría que darle una vuelta, ya que lo óptimo sería que en vez de dividir el texto del pdf en embeddigns, lo suyo sería que cada pdf fuese un embedding para que a la hora de hacerle las preguntas al rag, este busque el pdf de la convocatoria. No se si es mejor hacer un rag para esta parte(la de buscar el pdf exacto) y otro rag para resolver las preguntas que surjan dentro de ese pdf, o simplemente con un mismo rag es suficiente.\n",
        "\n",
        "De todas formas hay que darle una vuelta tanto a la salida de las preguntas como al modelo, ya que al ser un rag no es necesario hacer el fine tuning, por lo que podemos usar modelos bastante potentes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kLz-kxA8Hp6o",
      "metadata": {
        "id": "kLz-kxA8Hp6o"
      },
      "source": [
        "1/07 Neri\n",
        "\n",
        "Estuve haciendo pruebas para la búsqueda de convocatorias, la cual hice algunas funciones para que a partir de lenguaje natural se puedan hacer consultas de convocatorias,  recibe una query y genera un diccionario con los argumentos que despues se usan para solicitar el request con la API.\n",
        "\n",
        "En resumen este codigo tiene :\n",
        "*SubvencionesParser: Clase central que tiene la lógica para la extracción de parámetros.\n",
        "* Mapeo de Argumentos: Creé una serie de mapeos que asocian términos comunes en las consultas con los IDs o códigos numéricos correspondientes a cada categoría (e.g., regiones, tipos de beneficiario, actividades económicas, etc.).Acorde a la documentación de la API del sitio.\n",
        "* Extracción de parametros: La función extraer_parametros usa estos mapeos  para identificar  elementos como la descripción de la búsqueda, regiones, tipo de administración, beneficiarios, instrumentos, finalidades, actividades y fechas a partir del texto de la query.\n",
        "*Las funciones de mapeo y la estructura de la extracción la hice basándome en la documentación del sitio (compartida por Carmen).\n",
        "*Hice algunas funciones basadas en el trabajo de Carmen respecto a la comunicación con la API para bùsqueda de convocatorias, consulta especifica de una convocatoria y obtención del documento o documentos de una convocatoria.\n",
        "* añadí una parte del codigo para hacer un demo en  streamlit  para probar la extracción de argumentos de manera visual.\n",
        "\n",
        "Creo que lo siguiente que probaré será usar embeddings de spacy para clasificar los argumentos de manera más robusta , ya que estas funciones las hice a base de reglas, y si alguna palabra no luce exactamente igual puede que no la coja.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9632f49a",
      "metadata": {},
      "source": [
        "15/07 Por Camilo\n",
        "En resumen fue esto:    :)\n",
        "- Se ha reorganizado todo el proyecto en carpetas: `src/` para el código Python, `notebooks/` para los Jupyter, `data/` para los datos de entrada, y `outputs/` para resultados y PDFs.\n",
        "- Se ha añadido la carpeta `web/` con el frontend hecho en React.\n",
        "- Se han movido todos los archivos a su sitio correspondiente.\n",
        "- Se ha actualizado el `README.md` para documentar cómo ejecutar tanto el backend (Python) como el frontend (web), incluyendo instrucciones claras para ambos.\n",
        "- El frontend se ejecuta desde `web/project` con `npm install` y `npm run dev`.\n",
        "- El backend se ejecuta con `python src/rag.py` tras instalar dependencias.\n",
        "- Se ha dejado claro cómo conectar ambos sistemas si se quiere exponer una API.\n",
        "\n",
        "Próximos pasos sugeridos:\n",
        "- Implementar la API en Python (Flask/FastAPI) para conectar la web con el backend.\n",
        "- Documentar ejemplos de endpoints y flujos de integración."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffba90e",
      "metadata": {},
      "source": [
        "03/08 Óscar\n",
        "\n",
        "- Uní los códigos de Luis y de Alex para hacer la query inicial, descargar las convocatorias relevantes e implementar el RAG a una convocatoria seleccionada. \n",
        "- Edité algunas funciones existentes para crear un flujo más ordenado basado en clases con funciones. \n",
        "- Expandí un poco el prompt del agente, pero falta añadir más cosas para que sea más preciso ante distintas consultas. \n",
        "- Por hacer: \n",
        "1. Definir qué hacer cuándo un usuario está interesado en más de una consulta. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
